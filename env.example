# LLM Configuration
# Point to your provider of choice (e.g., OpenRouter, OpenAI, etc.)
# LLM_API_KEY=
# LLM_API_BASE_URL=https://openrouter.ai/api/v1
# LLM_MODEL=openai/gpt-4o-mini

# To use OpenAI directly, you would change the above to:
LLM_API_KEY=
LLM_API_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini

# LLM Behavior Configuration
# LLM_TEMPERATURE=0.0
# LLM_REQUEST_TIMEOUT=15

# API Configuration
TECH_TRIVIA_API_URL=https://opentdb.com/api.php?amount=1&category=18&type=multiple
FUN_FACTS_API_URL=https://uselessfacts.jsph.pl/random.json?language=en
GITHUB_TRENDING_URL=https://api.ossinsight.io/v1/trends/repos/

# Timeout Configuration (in seconds)
API_TIMEOUT=15

# Logging Configuration
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Server Configuration
MCP_HOST=127.0.0.1
MCP_PORT=8000
MCP_TRANSPORT=sse

# Optional: Langfuse Observability (https://langfuse.com)
# To enable, set these variables and run the local Langfuse server via Docker.
# If these are not set, Langfuse will be disabled.
LANGFUSE_SECRET_KEY=
LANGFUSE_PUBLIC_KEY=
LANGFUSE_HOST=http://localhost:3000

