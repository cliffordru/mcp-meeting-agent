# LLM Configuration
# Point to your provider of choice (e.g., OpenRouter, OpenAI, etc.)
# LLM_API_KEY=
# LLM_API_BASE_URL=https://openrouter.ai/api/v1
# LLM_MODEL=openai/gpt-4o-mini

# LLM Configuration
LLM_API_KEY=your_openai_or_openrouter_api_key_here
LLM_API_BASE_URL=https://api.openai.com/v1
LLM_MODEL=gpt-4o-mini

# LLM Behavior Configuration
# LLM_TEMPERATURE=0.0
# LLM_REQUEST_TIMEOUT=15

# API Configuration
TECH_TRIVIA_API_URL=https://opentdb.com/api.php?amount=1&category=18&type=multiple
FUN_FACTS_API_URL=https://uselessfacts.jsph.pl/random.json?language=en
GITHUB_TRENDING_URL=https://api.ossinsight.io/v1/trends/repos/

# Timeout Configuration (in seconds)
API_TIMEOUT=30
LLM_REQUEST_TIMEOUT=60
AGENT_EXECUTOR_TIMEOUT=120
MCP_TOOL_TIMEOUT=150

# Logging Configuration
# Options: DEBUG, INFO, WARNING, ERROR, CRITICAL
LOG_LEVEL=INFO

# Server Configuration
MCP_HOST=127.0.0.1
MCP_PORT=8000
MCP_TRANSPORT=sse

# Optional Langfuse Configuration (for observability)
LANGFUSE_SECRET_KEY=your_langfuse_secret_key_here
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key_here
LANGFUSE_HOST=https://cloud.langfuse.com

# FastMCP Configuration
MCP_MASK_ERROR_DETAILS=true
MCP_ENABLE_LOGGING=true

